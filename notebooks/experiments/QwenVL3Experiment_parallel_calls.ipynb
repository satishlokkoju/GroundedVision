{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2f6f57b",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d3dad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import base64\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from typing import List, Dict, Optional\n",
    "from openai import OpenAI\n",
    "\n",
    "from groundedvision.config import RAW_DATA_DIR, PROCESSED_DATA_DIR\n",
    "from tqdm import tqdm\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "logger.remove()  # Remove default handler\n",
    "logger.add(sys.stderr, level=\"DEBUG\", format=\" {level} | {message}\", colorize=True)\n",
    "\n",
    "class ConstructionProgressAnalyzer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        api_key: Optional[str] = None,\n",
    "        model: str = \"qwen3-vl-plus\",  # Options: \"qwen3-vl-plus\", \"qwen-vl-max\", \"qwen2-vl-72b-instruct\"\n",
    "        base_url: str = \"https://dashscope-us.aliyuncs.com/compatible-mode/v1\"\n",
    "    ):\n",
    "        self.api_key = api_key or os.environ.get(\"DASHSCOPE_API_KEY\")\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"API key must be provided via parameter or DASHSCOPE_API_KEY environment variable\")\n",
    "        \n",
    "        self.client = OpenAI(\n",
    "            api_key=self.api_key,\n",
    "            base_url=base_url.strip()  # Remove accidental trailing spaces\n",
    "        )\n",
    "        self.model = model\n",
    "        self.prompt = CONSTRUCTION_PROGRESS_PROMPT\n",
    "        self.face_order = [\"front\",\"right\", \"back\", \"left\",  \"top\", \"bottom\"]\n",
    "        self.max_retries = 3\n",
    "    \n",
    "    def encode_image(self, image_path: str) -> str:\n",
    "        \"\"\"Encode image to base64 data URL\"\"\"\n",
    "        try:\n",
    "            with open(image_path, \"rb\") as f:\n",
    "                encoded = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "            return f\"data:image/jpeg;base64,{encoded}\"\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to encode image {image_path}: {str(e)}\")\n",
    "    \n",
    "    def _clean_json_response(self, response_text: str) -> str:\n",
    "        \"\"\"Extract clean JSON from possible markdown wrappers or extra text\"\"\"\n",
    "        # Remove markdown code block markers\n",
    "        response_text = re.sub(r'^```json\\s*', '', response_text)\n",
    "        response_text = re.sub(r'\\s*```$', '', response_text)\n",
    "        \n",
    "        # Find first { and last } to extract JSON object\n",
    "        start = response_text.find('{')\n",
    "        end = response_text.rfind('}') + 1\n",
    "        \n",
    "        if start == -1 or end == 0:\n",
    "            raise ValueError(f\"Could not extract JSON from response: {response_text[:200]}...\")\n",
    "        \n",
    "        return response_text[start:end]\n",
    "    \n",
    "    def analyze_face_pair(\n",
    "        self,\n",
    "        img_a_path: str,\n",
    "        img_b_path: str,\n",
    "        face_id: str,\n",
    "        timeout: int = 120\n",
    "    ) -> Dict:\n",
    "        \"\"\"Analyze a single cubemap face pair using OpenAI-compatible API\"\"\"\n",
    "        try:\n",
    "            # Encode images\n",
    "            img_a_b64 = self.encode_image(img_a_path)\n",
    "            img_b_b64 = self.encode_image(img_b_path)\n",
    "            \n",
    "            # Build messages in OpenAI format\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": self.prompt\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": f\"FACE ID: {face_id}\\n\\nAnalyze the structural changes between these two images of the same construction site location taken at different times. Output STRICT VALID JSON ONLY.\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": \"Image A (Earlier Time)\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\": img_a_b64,\n",
    "                                \"detail\": \"high\"  # Request high detail for structural analysis\n",
    "                            }\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": \"Image B (Later Time)\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\": img_b_b64,\n",
    "                                \"detail\": \"high\"\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "            \n",
    "            # Retry logic with exponential backoff\n",
    "            for attempt in range(self.max_retries):\n",
    "                try:\n",
    "                    response = self.client.chat.completions.create(\n",
    "                        model=self.model,\n",
    "                        messages=messages,\n",
    "                        temperature=0.01,  # Minimize creativity\n",
    "                        max_tokens=1500,\n",
    "                        timeout=timeout\n",
    "                    )\n",
    "                    \n",
    "                    raw_content = response.choices[0].message.content.strip()\n",
    "                    logger.debug(f\"Raw API response for {face_id}: {raw_content[:300]}...\")\n",
    "                    \n",
    "                    # Clean and parse JSON\n",
    "                    clean_json = self._clean_json_response(raw_content)\n",
    "                    result = json.loads(clean_json)\n",
    "                    \n",
    "                    # Validate required fields\n",
    "                    required_fields = [\"structural_change_detected\", \"confidence_level\", \"change_description\"]\n",
    "                    if not all(field in result for field in required_fields):\n",
    "                        raise ValueError(f\"Missing required fields in JSON: {required_fields}\")\n",
    "                    \n",
    "                    # Enrich with metadata\n",
    "                    result[\"face_id\"] = face_id\n",
    "                    result[\"analysis_timestamp\"] = time.time()\n",
    "                    result[\"model_used\"] = self.model\n",
    "                    \n",
    "                    logger.info(f\"✓ Successfully analyzed {face_id} face (confidence: {result.get('confidence_level', 'N/A')})\")\n",
    "                    return result\n",
    "                    \n",
    "                except (json.JSONDecodeError, ValueError) as e:\n",
    "                    logger.warning(f\"Attempt {attempt+1}/{self.max_retries} failed for {face_id}: {str(e)}\")\n",
    "                    if attempt == self.max_retries - 1:\n",
    "                        return self._error_result(face_id, f\"JSON parsing failed after {self.max_retries} attempts: {str(e)}\")\n",
    "                    time.sleep(1.5 ** attempt)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"API error on attempt {attempt+1} for {face_id}: {str(e)}\")\n",
    "                    if attempt == self.max_retries - 1:\n",
    "                        return self._error_result(face_id, f\"API failed after {self.max_retries} attempts: {str(e)}\")\n",
    "                    time.sleep(2 ** attempt)\n",
    "            \n",
    "        except Exception as e:\n",
    "            return self._error_result(face_id, f\"Processing error: {str(e)}\")\n",
    "    \n",
    "    def _error_result(self, face_id: str, error_msg: str) -> Dict:\n",
    "        \"\"\"Generate standardized error result\"\"\"\n",
    "        logger.error(f\"Analysis failed for {face_id}: {error_msg}\")\n",
    "        return {\n",
    "            \"face_id\": face_id,\n",
    "            \"error\": error_msg,\n",
    "            \"structural_change_detected\": False,\n",
    "            \"confidence_level\": \"Low\",\n",
    "            \"change_description\": f\"Analysis failed: {error_msg}\",\n",
    "            \"registration_quality\": \"Low\",\n",
    "            \"transient_objects_ignored\": [],\n",
    "            \"specific_changes\": [],\n",
    "            \"potential_artifacts\": [error_msg]\n",
    "        }\n",
    "    \n",
    "    def consolidate_results(self, face_results: List[Dict]) -> Dict:\n",
    "        \"\"\"Aggregate face-level results into site-wide progress assessment\"\"\"\n",
    "        # Filter valid results\n",
    "        valid_results = [r for r in face_results if \"error\" not in r]\n",
    "        errored_faces = [r[\"face_id\"] for r in face_results if \"error\" in r]\n",
    "        logger.info(f\"Valid results: {len(valid_results)}, Errored faces: {len(errored_faces)}\")\n",
    "        # Collect high-confidence changes only\n",
    "        high_conf_changes = []\n",
    "        for res in valid_results:\n",
    "            logger.info(f\"Processing face {res['face_id']}, results {res}\")\n",
    "            if (res.get(\"structural_change_detected\") and \n",
    "                res.get(\"confidence_level\") == \"High\" and\n",
    "                res.get(\"specific_changes\")):\n",
    "                for change in res[\"specific_changes\"]:\n",
    "                    logger.info(f\"Processing change for {res['face_id']}: {change}\")\n",
    "                    if change.get(\"confidence\") == \"High\":\n",
    "                        high_conf_changes.append({\n",
    "                            **change,\n",
    "                            \"detected_in_face\": res[\"face_id\"],\n",
    "                            \"face_registration_quality\": res.get(\"registration_quality\", \"Medium\")\n",
    "                        })\n",
    "        \n",
    "        logger.info(f\"High confidence changes: {len(high_conf_changes)}\")\n",
    "        # Apply cross-face verification (critical for eliminating artifacts)\n",
    "        verified_changes = self._verify_cross_face_consistency(high_conf_changes, valid_results)\n",
    "        \n",
    "        # Determine progress direction\n",
    "        if not verified_changes:\n",
    "            direction = \"No Progress Detected\"\n",
    "        elif all(c[\"change_type\"] == \"Removed\" for c in verified_changes):\n",
    "            direction = \"Demolition/Removal\"\n",
    "        elif all(c[\"change_type\"] in [\"Added\", \"Modified\"] for c in verified_changes):\n",
    "            direction = \"Construction Added\"\n",
    "        else:\n",
    "            direction = \"Mixed Progress\"\n",
    "        \n",
    "        # Build consolidated description\n",
    "        change_description = self._generate_consolidated_description(verified_changes, valid_results)\n",
    "        \n",
    "        # Calculate overall confidence (conservative approach)\n",
    "        overall_confidence = \"High\" if len(verified_changes) >= 2 else (\n",
    "            \"Medium\" if len(verified_changes) == 1 else \"High\"  # High confidence in \"no change\" when all faces agree\n",
    "        )\n",
    "        \n",
    "        # Assess registration quality across all faces\n",
    "        registration_quality = self._assess_registration_quality(valid_results)\n",
    "        \n",
    "        return {\n",
    "            \"analysis_metadata\": {\n",
    "                \"timestamp\": time.time(),\n",
    "                \"model_used\": self.model,\n",
    "                \"total_faces_analyzed\": len(face_results),\n",
    "                \"successful_analyses\": len(valid_results),\n",
    "                \"errored_faces\": errored_faces,\n",
    "                \"face_order\": self.face_order\n",
    "            },\n",
    "            \"registration_quality_assessment\": registration_quality,\n",
    "            \"overall_confidence_level\": overall_confidence,\n",
    "            \"structural_change_detected\": len(verified_changes) > 0,\n",
    "            \"direction_of_progress\": direction,\n",
    "            \"change_description\": change_description,\n",
    "            \"consolidated_changes\": verified_changes,\n",
    "            \"transient_objects_summary\": self._aggregate_list_field(valid_results, \"transient_objects_ignored\"),\n",
    "            \"potential_artifacts_requiring_verification\": self._aggregate_list_field(valid_results, \"potential_artifacts\")[:5],\n",
    "            \"face_level_results\": face_results,  # For auditability\n",
    "            \"verification_protocol_applied\": [\n",
    "                \"Cross-face geometric consistency check\",\n",
    "                \">90% confidence threshold enforced per face\",\n",
    "                \"Transient objects explicitly filtered\",\n",
    "                \"Registration quality assessment across all faces\",\n",
    "                \"Construction sequence logic validation\"\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    def _verify_cross_face_consistency(self, changes: List[Dict], face_results: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Apply spatial reasoning to eliminate artifacts:\n",
    "        - Wall changes must appear in geometrically adjacent faces OR have unambiguous texture evidence\n",
    "        - Ceiling/floor changes must primarily appear in top/bottom faces\n",
    "        \"\"\"\n",
    "        if not changes:\n",
    "            return []\n",
    "        \n",
    "        # Cubemap adjacency map (which faces share edges)\n",
    "        adjacency_map = {\n",
    "            \"front\": [\"left\", \"right\", \"top\", \"bottom\"],\n",
    "            \"back\": [\"left\", \"right\", \"top\", \"bottom\"],\n",
    "            \"left\": [\"front\", \"back\", \"top\", \"bottom\"],\n",
    "            \"right\": [\"front\", \"back\", \"top\", \"bottom\"],\n",
    "            \"top\": [\"front\", \"back\", \"left\", \"right\"],\n",
    "            \"bottom\": [\"front\", \"back\", \"left\", \"right\"]\n",
    "        }\n",
    "        \n",
    "        verified = []\n",
    "        for change in changes:\n",
    "            element_type = change[\"element_type\"]\n",
    "            detected_face = change[\"detected_in_face\"]\n",
    "            logger.info(f\"Verifying change: {change} detected face {detected_face}\")\n",
    "            # Strong evidence markers that can override corroboration requirement\n",
    "            # Get evidence text - handle both old and new formats\n",
    "            evidence_field = change.get(\"supporting_evidence\", \"\")\n",
    "            if not evidence_field:\n",
    "                # New format uses texture_evidence as a list\n",
    "                texture_list = change.get(\"texture_evidence\", [])\n",
    "                evidence_field = \" \".join(texture_list) if isinstance(texture_list, list) else str(texture_list)\n",
    "\n",
    "            strong_evidence = any(kw in evidence_field.lower() \n",
    "                                for kw in [\"seam\", \"fastener\", \"screw\", \"joint\", \"grid\", \"conduit run\"])\n",
    "\n",
    "            logger.info(f\"Strong evidence: {strong_evidence}\")\n",
    "            # Apply verification rules\n",
    "            if element_type == \"Wall\":\n",
    "                # Wall changes should appear in adjacent faces OR have strong texture evidence\n",
    "                adjacent_faces = adjacency_map.get(detected_face, [])\n",
    "                corroborating_changes = [\n",
    "                    c for c in changes \n",
    "                    if c[\"element_type\"] == \"Wall\" \n",
    "                    and c[\"detected_in_face\"] in adjacent_faces\n",
    "                    and abs(self._face_distance(detected_face, c[\"detected_in_face\"])) <= 1\n",
    "                ]\n",
    "                \n",
    "                if not corroborating_changes and not strong_evidence:\n",
    "                    logger.warning(f\"Wall change in {detected_face} lacks corroboration - treating as artifact\")\n",
    "                    continue\n",
    "            \n",
    "            elif element_type == \"Ceiling\" and detected_face != \"top\":\n",
    "                # Ceiling changes should primarily be in top face\n",
    "                top_face_has_change = any(\n",
    "                    c[\"element_type\"] == \"Ceiling\" and c[\"detected_in_face\"] == \"top\"\n",
    "                    for c in changes\n",
    "                )\n",
    "                if not top_face_has_change and not strong_evidence:\n",
    "                    logger.warning(f\"Ceiling change detected outside top face without corroboration - treating as artifact\")\n",
    "                    continue\n",
    "            \n",
    "            elif element_type == \"Floor\" and detected_face != \"bottom\":\n",
    "                # Floor changes should primarily be in bottom face\n",
    "                bottom_face_has_change = any(\n",
    "                    c[\"element_type\"] == \"Floor\" and c[\"detected_in_face\"] == \"bottom\"\n",
    "                    for c in changes\n",
    "                )\n",
    "                if not bottom_face_has_change and not strong_evidence:\n",
    "                    logger.warning(f\"Floor change detected outside bottom face without corroboration - treating as artifact\")\n",
    "                    continue\n",
    "            \n",
    "            # Passed verification - add to verified changes\n",
    "            verified.append(change)\n",
    "        \n",
    "        return verified\n",
    "    \n",
    "    def _face_distance(self, face1: str, face2: str) -> int:\n",
    "        \"\"\"Calculate geometric distance between cube faces (0=same, 1=adjacent, 2=opposite)\"\"\"\n",
    "        opposites = {\"front\": \"back\", \"back\": \"front\", \"left\": \"right\", \"right\": \"left\", \"top\": \"bottom\", \"bottom\": \"top\"}\n",
    "        if face1 == face2:\n",
    "            return 0\n",
    "        elif opposites.get(face1) == face2:\n",
    "            return 2\n",
    "        else:\n",
    "            return 1\n",
    "    \n",
    "    def _assess_registration_quality(self, results: List[Dict]) -> str:\n",
    "        \"\"\"Assess overall registration quality based on face analyses\"\"\"\n",
    "        if not results:\n",
    "            return \"Low\"\n",
    "        \n",
    "        quality_counts = {\"High\": 0, \"Medium\": 0, \"Low\": 0}\n",
    "        for r in results:\n",
    "            quality = r.get(\"registration_quality\", \"Medium\")\n",
    "            if quality in quality_counts:\n",
    "                quality_counts[quality] += 1\n",
    "        \n",
    "        total = len(results)\n",
    "        if quality_counts[\"High\"] / total >= 0.7:\n",
    "            return \"High\"\n",
    "        elif quality_counts[\"Low\"] / total >= 0.5:\n",
    "            return \"Low\"\n",
    "        else:\n",
    "            return \"Medium\"\n",
    "    \n",
    "    def _generate_consolidated_description(self, changes: List[Dict], results: List[Dict]) -> str:\n",
    "        if not changes:\n",
    "            # Report verified unchanged elements\n",
    "            verified_elements = set()\n",
    "            for r in results:\n",
    "                if \"Verified:\" in r.get(\"change_description\", \"\"):\n",
    "                    parts = r[\"change_description\"].split(\"Verified:\")[1].strip()\n",
    "                    elements = [e.strip() for e in parts.split(\",\") if e.strip()]\n",
    "                    verified_elements.update(elements)\n",
    "            \n",
    "            if verified_elements:\n",
    "                elements_str = \", \".join(sorted(verified_elements))\n",
    "                return f\"No permanent structural changes detected. Verified unchanged across site: {elements_str}\"\n",
    "            else:\n",
    "                return \"No permanent structural changes detected. All visible structural elements remain unchanged.\"\n",
    "        \n",
    "        # Build change descriptions\n",
    "        descriptions = []\n",
    "        for change in changes:\n",
    "            loc = f\"{change['location']} ({change['detected_in_face']} face)\"\n",
    "            # Get evidence from either old or new field format\n",
    "            evidence = change.get('supporting_evidence', '')\n",
    "            if not evidence:\n",
    "                texture_list = change.get('texture_evidence', [])\n",
    "                evidence = ', '.join(texture_list) if isinstance(texture_list, list) else str(texture_list)\n",
    "            desc = f\"{change['element_type']} {change['change_type'].lower()}: {loc} [{evidence}]\"\n",
    "            descriptions.append(desc)\n",
    "        \n",
    "        return \" | \".join(descriptions)\n",
    "    \n",
    "    def _aggregate_list_field(self, results: List[Dict], field_name: str) -> List[str]:\n",
    "        \"\"\"Deduplicate and aggregate list fields from multiple results\"\"\"\n",
    "        items = []\n",
    "        for r in results:\n",
    "            items.extend(r.get(field_name, []))\n",
    "        # Deduplicate while preserving order\n",
    "        seen = set()\n",
    "        unique_items = []\n",
    "        for item in items:\n",
    "            if item not in seen:\n",
    "                seen.add(item)\n",
    "                unique_items.append(item)\n",
    "        return unique_items\n",
    "    \n",
    "    def analyze_site_progress(\n",
    "        self,\n",
    "        set_a_paths: List[str],\n",
    "        set_b_paths: List[str],\n",
    "        face_labels: Optional[List[str]] = None\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Main entry point: Analyze full cubemap sets and return consolidated progress report\n",
    "        \n",
    "        Args:\n",
    "            set_a_paths: List of 6 image paths (Time A) in order: [front, right, back, left, top, bottom]\n",
    "            set_b_paths: List of 6 image paths (Time B) in same order\n",
    "            face_labels: Optional custom labels for faces (defaults to standard cubemap order)\n",
    "        \n",
    "        Returns:\n",
    "            Consolidated JSON report of construction progress\n",
    "        \"\"\"\n",
    "        if len(set_a_paths) != 6 or len(set_b_paths) != 6:\n",
    "            raise ValueError(\n",
    "                \"Exactly 6 images required per set (cube faces in order: front, back, left, right, top, bottom). \"\n",
    "                f\"Received {len(set_a_paths)} for set A and {len(set_b_paths)} for set B.\"\n",
    "            )\n",
    "        \n",
    "        face_ids = face_labels if face_labels else self.face_order\n",
    "        \n",
    "        # Analyze each face pair sequentially\n",
    "        face_results = []\n",
    "        for i, face_id in enumerate(face_ids):\n",
    "            logger.info(f\"Analyzing {face_id} face ({i+1}/6)...\")\n",
    "            result = self.analyze_face_pair(set_a_paths[i], set_b_paths[i], face_id)\n",
    "            face_results.append(result)\n",
    "            time.sleep(0.3)  # Small delay to avoid rate limits\n",
    "        \n",
    "        # Consolidate into site-wide assessment\n",
    "        consolidated = self.consolidate_results(face_results)\n",
    "        return consolidated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea18521",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONSTRUCTION_PROGRESS_PROMPT = \"\"\"ROLE: You are a Construction Progress Monitoring AI. Detect PERMANENT structural changes between two construction site images. Balance precision with recall - catch real changes while avoiding false positives.\n",
    "\n",
    "INPUT CONTEXT:\n",
    "- You are analyzing ONE PAIR of cubemap faces from the same viewing direction at different times.\n",
    "- The FACE ID will be provided in the user message.\n",
    "- Images come from 360° cameras with IMPERFECT registration (expect ≤5° misalignment, perspective shifts, lighting differences).\n",
    "- THIS IS A SINGLE FACE ANALYSIS. Do not assume knowledge from other faces.\n",
    "\n",
    "VERIFICATION PROTOCOL:\n",
    "\n",
    "Step 0 - DESCRIBE BEFORE COMPARE (MANDATORY):\n",
    "Describe each image independently BEFORE looking for differences:\n",
    "  • Image A: \"I see [structural elements] at [locations]...\"\n",
    "  • Image B: \"I see [structural elements] at [locations]...\"\n",
    "\n",
    "Step 1 - IDENTIFY CANDIDATE CHANGES:\n",
    "Compare your descriptions. A change exists ONLY if:\n",
    "  - An element is ABSENT in Image A AND PRESENT in Image B (or vice versa)\n",
    "  - You can identify the EXACT region where the difference occurs\n",
    "\n",
    "Step 2 - ELIMINATE ARTIFACTS:\n",
    "For each candidate change, ask: \"Could this be explained by camera angle, lighting, shadows, or reflections?\"\n",
    "  • Same surface appearing brighter/darker = lighting, NOT structural\n",
    "  • Same element at different angle = perspective, NOT structural\n",
    "  • Reflections on metal/glass = artifact, NOT structural\n",
    "  → If YES = discard. Only proceed if definitively NO.\n",
    "\n",
    "Step 3 - VERIFY WITH ELEMENT-SPECIFIC MARKERS:\n",
    "Each element type has specific visual markers. Identify at least ONE marker from the appropriate category:\n",
    "\n",
    "WALLS & PARTITIONS:\n",
    "  • Drywall/Gypsum: Panel seams, screw patterns, tape joints, uniform matte finish\n",
    "  • Metal studs: C-channel profile, consistent vertical spacing, track connections\n",
    "  • Concrete block: Mortar lines, block pattern, grey uniform texture\n",
    "\n",
    "MEP SYSTEMS (Mechanical, Electrical, Plumbing):\n",
    "  • HVAC Ductwork: Rectangular/round shapes, foil insulation wrap, support hangers, flex connections\n",
    "  • Fire suppression: Copper/red pipes, T-joints, sprinkler heads, ceiling brackets\n",
    "  • Electrical conduit: Metal/PVC tubing, junction boxes, support clips, corner bends\n",
    "  • Plumbing: PVC/copper pipes, elbows, valves, floor/wall penetrations\n",
    "\n",
    "CEILING SYSTEMS:\n",
    "  • Drop ceiling grid: T-bar intersections, consistent pattern, hanging wires\n",
    "  • Insulation: Batts between joists, foil-faced products, vapor barriers\n",
    "  • Exposed structure: Deck, joists, beams becoming visible/covered\n",
    "\n",
    "FLOOR SYSTEMS:\n",
    "  • Concrete: Pour lines, control joints, finish differences, form marks\n",
    "  • Coatings: Epoxy sheen, color changes, edge lines\n",
    "\n",
    "SAFETY & ACCESS:\n",
    "  • Guardrails/Handrails: Metal posts at regular intervals, horizontal rails, floor brackets\n",
    "  • Stairs: Treads, risers, stringers, landing platforms\n",
    "  • Temporary barriers: Plywood, caution tape, signage\n",
    "\n",
    "Step 4 - CONSTRUCTION LOGIC CHECK:\n",
    "Does this change make sense in construction sequence?\n",
    "  • New elements appear with proper supports/connections\n",
    "  • Changes are consistent across the visible area\n",
    "  • Logical progression (framing → MEP rough-in → insulation → drywall)\n",
    "\n",
    "STRICT EXCLUSIONS (NEVER REPORT):\n",
    "✗ Equipment movement (lifts, carts, scaffolding positions)\n",
    "✗ Worker positions, tools, personal items\n",
    "✗ Loose materials, debris, temporary storage\n",
    "✗ Lighting/shadow differences\n",
    "✗ Camera angle/perspective variations\n",
    "✗ Reflections, glare, motion blur, compression artifacts\n",
    "\n",
    "DECISION THRESHOLD:\n",
    "• structural_change_detected = true if:\n",
    "  1. Element is clearly absent in one image and present in the other\n",
    "  2. At least ONE element-specific marker is identified\n",
    "  3. Change passes artifact elimination (Step 2)\n",
    "  4. Change makes construction sequence sense\n",
    "\n",
    "• When genuinely uncertain → default to FALSE with explanation\n",
    "• Report real changes; avoid false positives\n",
    "\n",
    "OUTPUT FORMAT (STRICT JSON ONLY - no markdown):\n",
    "{\n",
    "  \"face_id\": \"<face name from user message>\",\n",
    "  \"image_a_description\": \"Brief structural element inventory of Image A\",\n",
    "  \"image_b_description\": \"Brief structural element inventory of Image B\", \n",
    "  \"registration_quality\": \"High|Medium|Low\",\n",
    "  \"confidence_level\": \"High|Medium|Low\",\n",
    "  \"transient_objects_ignored\": [\"list of transient objects seen but ignored\"],\n",
    "  \"structural_change_detected\": true|false,\n",
    "  \"change_description\": \"Technical summary OR 'No structural changes detected. Both images show: [elements]'\",\n",
    "  \"specific_changes\": [\n",
    "    {\n",
    "      \"element_type\": \"Wall|Ceiling|MEP|Floor|Safety|Other\",\n",
    "      \"element_subtype\": \"e.g., 'drywall', 'guardrail', 'copper pipe'\",\n",
    "      \"location\": \"specific location in image\",\n",
    "      \"change_type\": \"Added|Removed|Modified\",\n",
    "      \"confidence\": \"High|Medium|Low\",\n",
    "      \"absence_in_image_a\": \"What was at this location in Image A\",\n",
    "      \"presence_in_image_b\": \"What is now at this location in Image B\",\n",
    "      \"visual_markers\": [\"specific markers observed, e.g., 'metal posts at 4ft spacing', 'horizontal rail']\"\n",
    "    }\n",
    "  ],\n",
    "  \"potential_artifacts\": [\"lighting/angle differences noted but NOT reported as changes\"]\n",
    "}\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaa2d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['DASHSCOPE_API_KEY'] = 'sk-1234567890' # Your DashScope API Key\n",
    "# Initialize analyzer\n",
    "analyzer = ConstructionProgressAnalyzer(\n",
    "    api_key=os.environ.get(\"DASHSCOPE_API_KEY\"),\n",
    "    model=\"qwen3-vl-plus\"  # Use \"qwen-vl-max\" for highest accuracy if budget allows\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070c2608",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f'{PROCESSED_DATA_DIR}/matched_pairs_json'\n",
    "list_of_aligned_directories = [d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3a22e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def process_folder(folder_path, analyzer, PROCESSED_DATA_DIR, logger):\n",
    "    \"\"\"Process a single folder - this runs in parallel.\"\"\"\n",
    "    input_folder_path = f\"{PROCESSED_DATA_DIR}/matched_pairs_json/{folder_path}/cubemap_cross/\"\n",
    "    final_output_path = f\"{PROCESSED_DATA_DIR}/matched_pairs_json/{folder_path}/construction_progress_report_qwen3_vl_plus.json\"\n",
    "\n",
    "    if os.path.exists(final_output_path):\n",
    "        with open(final_output_path, 'r') as f:\n",
    "            report = json.load(f)\n",
    "        return {\"folder\": folder_path, \"status\": \"success\", \"report\": report}\n",
    "    \n",
    "    new_panorama_frame = None\n",
    "    old_panorama_frame = None\n",
    "    \n",
    "    for image_path in os.listdir(input_folder_path):\n",
    "        full_path = os.path.join(input_folder_path, image_path)\n",
    "        path_full_path = Path(full_path)\n",
    "        \n",
    "        if \"cubemap_cross_collage.jpg\" in full_path:\n",
    "            logger.info(f\"Processing {path_full_path.stem}\")\n",
    "            filename = path_full_path.stem\n",
    "            if filename.startswith(\"new_\"):\n",
    "                stripped = filename[4:]\n",
    "                new_panorama_frame = stripped.split(\"_aligned\")[0]\n",
    "            if filename.startswith(\"old_\"):\n",
    "                stripped = filename[4:]\n",
    "                old_panorama_frame = stripped.split(\"_aligned\")[0]\n",
    "    new_faces_paths = []\n",
    "    old_faces_paths = []\n",
    "    for i in range(0, 6):\n",
    "        new_face_name = os.path.join(input_folder_path, f\"new_{new_panorama_frame}_aligned_{i}.jpg\")\n",
    "        old_face_name = os.path.join(input_folder_path, f\"old_{old_panorama_frame}_aligned_{i}.jpg\")\n",
    "        if not os.path.exists(new_face_name):\n",
    "            logger.error(f\"The file doesnt exist: {new_face_name}\")\n",
    "        if not os.path.exists(old_face_name):\n",
    "            logger.error(f\"The file doesnt exist: {old_face_name}\")\n",
    "        new_faces_paths.append(new_face_name)\n",
    "        old_faces_paths.append(old_face_name)\n",
    "    \n",
    "    try:\n",
    "        report = analyzer.analyze_site_progress(\n",
    "            old_faces_paths, new_faces_paths, \n",
    "            [\"front\", \"right\", \"back\", \"left\", \"top\", \"bottom\"]\n",
    "        )\n",
    "        \n",
    "        with open(final_output_path, \"w\") as f:\n",
    "            json.dump(report, f, indent=2)\n",
    "        \n",
    "        logger.debug(f\"\\nFull report saved to: {final_output_path}\")\n",
    "        return {\"folder\": folder_path, \"status\": \"success\", \"report\": report}\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Analysis failed for {folder_path}: {str(e)}\")\n",
    "        return {\"folder\": folder_path, \"status\": \"failed\", \"error\": str(e)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c508a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main parallel execution\n",
    "max_workers = 4  # Adjust based on API rate limits\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    # Submit all tasks\n",
    "    futures = {\n",
    "        executor.submit(process_folder, folder_path, analyzer, PROCESSED_DATA_DIR, logger): folder_path \n",
    "        for folder_path in list_of_aligned_directories\n",
    "    }\n",
    "    \n",
    "    # Process results as they complete\n",
    "    for future in as_completed(futures):\n",
    "        folder_path = futures[future]\n",
    "        try:\n",
    "            result = future.result()\n",
    "            if result[\"status\"] == \"success\":\n",
    "                logger.info(f\"✅ Completed: {folder_path}\")\n",
    "            else:\n",
    "                logger.error(f\"❌ Failed: {folder_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Exception for {folder_path}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "track3d_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
